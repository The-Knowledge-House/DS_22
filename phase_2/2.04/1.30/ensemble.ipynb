{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "\n",
    "The following notebook is a combination of guided exploration of the random forest model using our hr dataset.\n",
    "\n",
    "Your job is to fill in the missing code-blocks and analyze the outputs that your code produces.\n",
    "\n",
    "Credit to [fares-ds](https://github.com/fares-ds) for the notebook.\n",
    "\n",
    "Modifications to instructions and code are indicated via a note from me: (*Note*: ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "Decision Trees are an important type of algorithm for predictive modeling machine learning.\n",
    "\n",
    "The classical decision tree algorithms have been around for decades and modern variations like random forest are among the most powerful techniques available.\n",
    "\n",
    "Classification and Regression Trees or CART for short is a term introduced by Leo Breiman to refer to Decision Tree algorithms that can be used for classification or regression predictive modeling problems.\n",
    "\n",
    "Classically, this algorithm is referred to as “decision trees”, but on some platforms like R they are referred to by the more modern term CART.\n",
    "\n",
    "The CART algorithm provides a foundation for important algorithms like bagged decision trees, random forest and boosted decision trees.\n",
    "\n",
    "**CART Model Representation**\n",
    "\n",
    "The representation for the CART model is a binary tree.\n",
    "\n",
    "This is your binary tree from algorithms and data structures, nothing too fancy. Each root node represents a single input variable (x) and a split point on that variable (assuming the variable is numeric).\n",
    "\n",
    "The leaf nodes of the tree contain an output variable (y) which is used to make a prediction.\n",
    "\n",
    "Given a new input, the tree is traversed by evaluating the specific input started at the root node of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros \n",
    "\n",
    "* Simple to understand and to interpret. Trees can be visualised.  \n",
    "* Requires little data preparation.  \n",
    "* Able to handle both numerical and categorical data. \n",
    "* Possible to validate a model using statistical tests.  \n",
    "* Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cons\n",
    "\n",
    "* Overfitting. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.  \n",
    "* Decision trees can be unstable. Mitigant: Use decision trees within an ensemble.  \n",
    "* Cannot guarantee to return the globally optimal decision tree. Mitigant: Training multiple trees in an ensemble learner  \n",
    "* Decision tree learners create biased trees if some classes dominate. Recommendation: Balance the dataset prior to fitting  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Random Forest is one of the most popular and most powerful machine learning algorithms. It is a type of ensemble machine learning algorithm called Bootstrap Aggregation or bagging.\n",
    "\n",
    "To improve performance of Decision trees, we can use many trees with a random sample of features chosen as the split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree & Random Forest Implementation in python\n",
    "\n",
    "We will use Decision Tree & Random Forest in predicting the attrition of your valuable employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from IPython.display import Image\n",
    "from six import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload 'hr' dataset\n",
    "df = pd.read_csv(\"hr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>EnvironmentSatisfaction</th>\n",
       "      <th>Gender</th>\n",
       "      <th>...</th>\n",
       "      <th>PerformanceRating</th>\n",
       "      <th>RelationshipSatisfaction</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1102</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>279</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1373</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1392</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>591</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Attrition  BusinessTravel  DailyRate  Department  DistanceFromHome  \\\n",
       "0   41          1               2       1102           2                 1   \n",
       "1   49          0               1        279           1                 8   \n",
       "2   37          1               2       1373           1                 2   \n",
       "3   33          0               1       1392           1                 3   \n",
       "4   27          0               2        591           1                 2   \n",
       "\n",
       "   Education  EducationField  EnvironmentSatisfaction  Gender  ...  \\\n",
       "0          2               1                        2       0  ...   \n",
       "1          1               1                        3       1  ...   \n",
       "2          2               4                        4       1  ...   \n",
       "3          4               1                        4       0  ...   \n",
       "4          1               3                        1       1  ...   \n",
       "\n",
       "   PerformanceRating  RelationshipSatisfaction  StockOptionLevel  \\\n",
       "0                  3                         1                 0   \n",
       "1                  4                         4                 1   \n",
       "2                  3                         2                 0   \n",
       "3                  3                         3                 0   \n",
       "4                  3                         4                 1   \n",
       "\n",
       "   TotalWorkingYears  TrainingTimesLastYear  WorkLifeBalance  YearsAtCompany  \\\n",
       "0                  8                      0                1               6   \n",
       "1                 10                      3                3              10   \n",
       "2                  7                      3                3               0   \n",
       "3                  8                      3                3               8   \n",
       "4                  6                      3                3               2   \n",
       "\n",
       "   YearsInCurrentRole  YearsSinceLastPromotion  YearsWithCurrManager  \n",
       "0                   4                        0                     5  \n",
       "1                   7                        1                     7  \n",
       "2                   0                        0                     0  \n",
       "3                   7                        3                     0  \n",
       "4                   2                        2                     2  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out the first 5 rows of your dataframe\n",
    "df._ _ _()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saidmf\\anaconda3\\envs\\phase1\\lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "c:\\Users\\saidmf\\anaconda3\\envs\\phase1\\lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "c:\\Users\\saidmf\\anaconda3\\envs\\phase1\\lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Attrition', ylabel='count'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a countplot of the \"Attrition\" column in this dataframe to view if classes are balanced\n",
    "sns.countplot(x='_ _ _', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the \"EmployeeCount\", 'EmployeeNumber', 'Over18', 'StandardHours' hours along the column axis \n",
    "df.drop(['_ _ _', '_ _ _', '_ _ _', '_ _ _'], axis=\"columns\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of categorical columns where unique samples are less than 50\n",
    " \n",
    "categorical_col = []\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == object and len(df[column].unique()) <= 50:\n",
    "        categorical_col.append(column)\n",
    "        \n",
    "df['Attrition'] = df.Attrition.astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the 'Attrition' column from the 'categorical_col' \n",
    "categorical_col.remove('_ _ _')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform select categorical columns into dummies\n",
    "label = LabelEncoder()\n",
    "for column in categorical_col:\n",
    "    df[column] = label.fit_transform(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 'X' and 'Y' training set, where 'X' is simply the dataframe without the 'Attrition' column\n",
    "X = df.drop('_ _ _', axis=1)\n",
    "\n",
    "# and 'Y' is the 'Attrition' column\n",
    "y = df['_ _ _']\n",
    "\n",
    "# create 'train_test_split' splits on the X and Y data where the test_size is 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(_, _, test_size=_, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a `print_score` method to generate a report\n",
    " \n",
    "def print_score(clf, X_train, y_train, X_test, y_test, train=True):\n",
    "    if train:\n",
    "        pred = clf.predict(X_train)\n",
    "        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n",
    "        print(\"Train Result:\\n================================================\")\n",
    "        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n",
    "        \n",
    "    elif train==False:\n",
    "        pred = clf.predict(X_test)\n",
    "        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n",
    "        print(\"Test Result:\\n================================================\")        \n",
    "        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "\n",
    "Decision Tree parameters:\n",
    "\n",
    "criterion: The function to measure the quality of a split. Supported criteria are \"gini\" for the Gini impurity and \"entropy\" for the information gain.  \n",
    "\n",
    "splitter: The strategy used to choose the split at each node. Supported strategies are \"best\" to choose the best split and \"random\" to choose the best random split.  \n",
    "\n",
    "max_depth: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.  \n",
    "\n",
    "min_samples_split: The minimum number of samples required to split an internal node.  \n",
    "\n",
    "min_samples_leaf: The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.  \n",
    "\n",
    "min_weight_fraction_leaf: The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.  \n",
    "\n",
    "max_features: The number of features to consider when looking for the best split.  \n",
    "\n",
    "max_leaf_nodes: Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.  \n",
    "\n",
    "min_impurity_decrease: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "min_impurity_split: Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple DecisionTreeClassifier object\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# fit on X_train & y_train data\n",
    "tree_clf.fit(_ _ _, _ _ _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 86.78%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                    0           1  accuracy    macro avg  weighted avg\n",
      "precision    0.887568    0.692308  0.867833     0.789938      0.854170\n",
      "recall       0.962485    0.409091  0.867833     0.685788      0.867833\n",
      "f1-score     0.923510    0.514286  0.867833     0.718898      0.853516\n",
      "support    853.000000  176.000000  0.867833  1029.000000   1029.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[821  32]\n",
      " [104  72]]\n",
      "\n",
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 87.30%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                    0          1  accuracy   macro avg  weighted avg\n",
      "precision    0.891304   0.592593  0.873016    0.741948      0.849986\n",
      "recall       0.971053   0.262295  0.873016    0.616674      0.873016\n",
      "f1-score     0.929471   0.363636  0.873016    0.646554      0.851204\n",
      "support    380.000000  61.000000  0.873016  441.000000    441.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[369  11]\n",
      " [ 45  16]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the report of the DecisionTree performance\n",
    "print_score(tree_clf, X_train, y_train, X_test, y_test, train=True)\n",
    "print_score(tree_clf, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4332 candidates, totalling 21660 fits\n",
      "Best paramters: {'criterion': 'entropy', 'max_depth': 6, 'min_samples_leaf': 19, 'min_samples_split': 2, 'splitter': 'best'})\n",
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 86.78%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                    0           1  accuracy    macro avg  weighted avg\n",
      "precision    0.887568    0.692308  0.867833     0.789938      0.854170\n",
      "recall       0.962485    0.409091  0.867833     0.685788      0.867833\n",
      "f1-score     0.923510    0.514286  0.867833     0.718898      0.853516\n",
      "support    853.000000  176.000000  0.867833  1029.000000   1029.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[821  32]\n",
      " [104  72]]\n",
      "\n",
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 87.30%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                    0          1  accuracy   macro avg  weighted avg\n",
      "precision    0.891304   0.592593  0.873016    0.741948      0.849986\n",
      "recall       0.971053   0.262295  0.873016    0.616674      0.873016\n",
      "f1-score     0.929471   0.363636  0.873016    0.646554      0.851204\n",
      "support    380.000000  61.000000  0.873016  441.000000    441.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[369  11]\n",
      " [ 45  16]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# search for the best possible hyperparameters, using GridSearchCV\n",
    "# NOTE: This might take some time\n",
    "\n",
    "params = {\n",
    "    \"criterion\":(\"gini\", \"entropy\"), \n",
    "    \"splitter\":(\"best\", \"random\"), \n",
    "    \"max_depth\":(list(range(1, 20))), \n",
    "    \"min_samples_split\":[2, 3, 4], \n",
    "    \"min_samples_leaf\":list(range(1, 20)), \n",
    "}\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "# TODO: Implement `GridSearchCV` object\n",
    "tree_cv = _ _ _(\n",
    "    tree_clf, \n",
    "    params, \n",
    "    scoring=\"f1\", \n",
    "    n_jobs=-1, \n",
    "    verbose=1, \n",
    "    cv=5\n",
    ")\n",
    "\n",
    "tree_cv.fit(X_train, y_train)\n",
    "best_params = tree_cv.best_params_\n",
    "print(f\"Best paramters: {best_params})\")\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(**best_params)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "print_score(tree_clf, X_train, y_train, X_test, y_test, train=True)\n",
    "print_score(tree_clf, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "\n",
    "Random forest algorithm parameters:\n",
    "\n",
    "n_estimators: The number of trees in the forest.  \n",
    "\n",
    "criterion: The function to measure the quality of a split. Supported criteria are \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
    "max_depth: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.  \n",
    " \n",
    "min_samples_split: The minimum number of samples required to split an internal node.  \n",
    "\n",
    "min_samples_leaf: The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.  \n",
    "\n",
    "min_weight_fraction_leaf: The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.  \n",
    "\n",
    "max_features: The number of features to consider when looking for the best split.  \n",
    "\n",
    "max_leaf_nodes: Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.  \n",
    "\n",
    "min_impurity_decrease: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.  \n",
    "\n",
    "min_impurity_split: Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.  \n",
    "\n",
    "bootstrap: Whether bootstrap samples are used when building trees. If False, the whole datset is used to build each tree.  \n",
    "\n",
    "oob_score: Whether to use out-of-bag samples to estimate the generalization accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 'RandomForestClassifier' \n",
    "\n",
    "rf_clf = _ _ _(n_estimators=100)\n",
    "\n",
    "# fit the randomforest object using the X_train & y_train data\n",
    "rf_clf.fit(_ _ _, _ _ _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a report on the random forest model\n",
    "\n",
    "print_score(rf_clf, X_train, y_train, X_test, y_test, train=True)\n",
    "print_score(rf_clf, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for the best possible hyperparameters, using RandomSearchCV\n",
    "# NOTE: This might take some time\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "random_grid = {\n",
    "    'n_estimators': n_estimators, \n",
    "    'max_features': max_features,\n",
    "    'max_depth': max_depth, \n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf, \n",
    "    'bootstrap': bootstrap\n",
    "}\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "# TODO: Implement `RandomizedSearchCV` object\n",
    "rf_cv = _ _ _(\n",
    "    estimator=rf_clf, \n",
    "    scoring='f1',\n",
    "    param_distributions=random_grid, \n",
    "    n_iter=200, \n",
    "    cv=5, \n",
    "    verbose=1, \n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_cv.fit(X_train, y_train)\n",
    "rf_best_params = rf_cv.best_params_\n",
    "print(f\"Best paramters: {rf_best_params})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(**rf_best_params)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "print_score(rf_clf, X_train, y_train, X_test, y_test, train=True)\n",
    "print_score(rf_clf, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "In this notebook we learned the following lessons:\n",
    "\n",
    "* Decision tree and random forest algorithms and the parameters of each algorithm.  \n",
    "* How to tune hyperparameters for both Decision tree and Random Forest.  \n",
    "* Balance your dataset before training to prevent the tree from being biased toward the classes that are dominant.\n",
    "    * By sampling an equal number of samples from each class\n",
    "    * By normalizing the sum of the sample weights (sample_weight) for each class to the same value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phase1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
